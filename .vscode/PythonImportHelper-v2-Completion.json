[
    {
        "label": "pandas",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pandas",
        "description": "pandas",
        "detail": "pandas",
        "documentation": {}
    },
    {
        "label": "sys",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "sys",
        "description": "sys",
        "detail": "sys",
        "documentation": {}
    },
    {
        "label": "os",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os",
        "description": "os",
        "detail": "os",
        "documentation": {}
    },
    {
        "label": "re",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "re",
        "description": "re",
        "detail": "re",
        "documentation": {}
    },
    {
        "label": "argparse",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "argparse",
        "description": "argparse",
        "detail": "argparse",
        "documentation": {}
    },
    {
        "label": "numpy",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "numpy",
        "description": "numpy",
        "detail": "numpy",
        "documentation": {}
    },
    {
        "label": "seaborn",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "seaborn",
        "description": "seaborn",
        "detail": "seaborn",
        "documentation": {}
    },
    {
        "label": "matplotlib.pyplot",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "matplotlib.pyplot",
        "description": "matplotlib.pyplot",
        "detail": "matplotlib.pyplot",
        "documentation": {}
    },
    {
        "label": "defaultdict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "defaultdict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "defaultdict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "spearmanr",
        "importPath": "scipy.stats",
        "description": "scipy.stats",
        "isExtraImport": true,
        "detail": "scipy.stats",
        "documentation": {}
    },
    {
        "label": "kendalltau",
        "importPath": "scipy.stats",
        "description": "scipy.stats",
        "isExtraImport": true,
        "detail": "scipy.stats",
        "documentation": {}
    },
    {
        "label": "kendalltau",
        "importPath": "scipy.stats",
        "description": "scipy.stats",
        "isExtraImport": true,
        "detail": "scipy.stats",
        "documentation": {}
    },
    {
        "label": "csv",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "csv",
        "description": "csv",
        "detail": "csv",
        "documentation": {}
    },
    {
        "label": "networkx",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "networkx",
        "description": "networkx",
        "detail": "networkx",
        "documentation": {}
    },
    {
        "label": "KFold",
        "importPath": "sklearn.model_selection",
        "description": "sklearn.model_selection",
        "isExtraImport": true,
        "detail": "sklearn.model_selection",
        "documentation": {}
    },
    {
        "label": "write_list_of_lists_to_file",
        "kind": 2,
        "importPath": "data.scripts.format",
        "description": "data.scripts.format",
        "peekOfCode": "def write_list_of_lists_to_file(data, output_file):\n    with open(output_file, 'w') as f:\n        for inner_list in data:\n            f.write(' '.join(map(str, inner_list)) + '\\n')\ndef reformat(input_file, output_file, mapping_file):\n    # Load the CSV file into Python\n    data = pd.read_csv(input_file, sep=';')\n    # Create new columns based on distinct values in the \"class\" column\n    distinct_values = data['class'].unique()\n    for value in distinct_values:",
        "detail": "data.scripts.format",
        "documentation": {}
    },
    {
        "label": "reformat",
        "kind": 2,
        "importPath": "data.scripts.format",
        "description": "data.scripts.format",
        "peekOfCode": "def reformat(input_file, output_file, mapping_file):\n    # Load the CSV file into Python\n    data = pd.read_csv(input_file, sep=';')\n    # Create new columns based on distinct values in the \"class\" column\n    distinct_values = data['class'].unique()\n    for value in distinct_values:\n        column_name = f'class_{value}'\n        data[column_name] = (data['class'] == value).astype(int)\n    # Remove the original \"class\" column\n    data.drop(columns=['class'], inplace=True)",
        "detail": "data.scripts.format",
        "documentation": {}
    },
    {
        "label": "input_csv_file",
        "kind": 5,
        "importPath": "data.scripts.format",
        "description": "data.scripts.format",
        "peekOfCode": "input_csv_file = sys.argv[1]\noutput_dat_file = sys.argv[2]\nmapping_file = sys.argv[3]\n# Call the function to convert CSV to DAT\nreformat(input_csv_file, output_dat_file, mapping_file)",
        "detail": "data.scripts.format",
        "documentation": {}
    },
    {
        "label": "output_dat_file",
        "kind": 5,
        "importPath": "data.scripts.format",
        "description": "data.scripts.format",
        "peekOfCode": "output_dat_file = sys.argv[2]\nmapping_file = sys.argv[3]\n# Call the function to convert CSV to DAT\nreformat(input_csv_file, output_dat_file, mapping_file)",
        "detail": "data.scripts.format",
        "documentation": {}
    },
    {
        "label": "mapping_file",
        "kind": 5,
        "importPath": "data.scripts.format",
        "description": "data.scripts.format",
        "peekOfCode": "mapping_file = sys.argv[3]\n# Call the function to convert CSV to DAT\nreformat(input_csv_file, output_dat_file, mapping_file)",
        "detail": "data.scripts.format",
        "documentation": {}
    },
    {
        "label": "write_list_of_lists_to_file",
        "kind": 2,
        "importPath": "data.format",
        "description": "data.format",
        "peekOfCode": "def write_list_of_lists_to_file(data, output_file):\n    with open(output_file, 'w') as f:\n        for inner_list in data:\n            f.write(' '.join(map(str, inner_list)) + '\\n')\ndef reformat(input_file, output_file, mapping_file):\n    # Load the CSV file into Python\n    data = pd.read_csv(input_file, sep=';')\n    # Create new columns based on distinct values in the \"class\" column\n    distinct_values = data['class'].unique()\n    for value in distinct_values:",
        "detail": "data.format",
        "documentation": {}
    },
    {
        "label": "reformat",
        "kind": 2,
        "importPath": "data.format",
        "description": "data.format",
        "peekOfCode": "def reformat(input_file, output_file, mapping_file):\n    # Load the CSV file into Python\n    data = pd.read_csv(input_file, sep=';')\n    # Create new columns based on distinct values in the \"class\" column\n    distinct_values = data['class'].unique()\n    for value in distinct_values:\n        column_name = f'class_{value}'\n        data[column_name] = (data['class'] == value).astype(int)\n    # Remove the original \"class\" column\n    data.drop(columns=['class'], inplace=True)",
        "detail": "data.format",
        "documentation": {}
    },
    {
        "label": "input_csv_file",
        "kind": 5,
        "importPath": "data.format",
        "description": "data.format",
        "peekOfCode": "input_csv_file = sys.argv[1]\noutput_dat_file = sys.argv[2]\nmapping_file = sys.argv[3]\n# Call the function to convert CSV to DAT\nreformat(input_csv_file, output_dat_file, mapping_file)",
        "detail": "data.format",
        "documentation": {}
    },
    {
        "label": "output_dat_file",
        "kind": 5,
        "importPath": "data.format",
        "description": "data.format",
        "peekOfCode": "output_dat_file = sys.argv[2]\nmapping_file = sys.argv[3]\n# Call the function to convert CSV to DAT\nreformat(input_csv_file, output_dat_file, mapping_file)",
        "detail": "data.format",
        "documentation": {}
    },
    {
        "label": "mapping_file",
        "kind": 5,
        "importPath": "data.format",
        "description": "data.format",
        "peekOfCode": "mapping_file = sys.argv[3]\n# Call the function to convert CSV to DAT\nreformat(input_csv_file, output_dat_file, mapping_file)",
        "detail": "data.format",
        "documentation": {}
    },
    {
        "label": "parse_filename",
        "kind": 2,
        "importPath": "scripts.plot_active_learning",
        "description": "scripts.plot_active_learning",
        "peekOfCode": "def parse_filename(filename):\n    \"\"\"\n    Parses the filename to extract datasetName, foldID, LearningAlgorithm, Oracle, and timestamp.\n    Expected filename format: datasetName_foldID_LearningAlgorithm_Oracle_timestamp.csv\n    \"\"\"\n    basename = os.path.basename(filename)\n    pattern = r'^(.*?)_(.*?)_(.*?)_(.*?)_(.*?)\\.csv$'\n    match = re.match(pattern, basename)\n    if match:\n        datasetName, foldID, LearningAlgorithm, Oracle, timestamp = match.groups()",
        "detail": "scripts.plot_active_learning",
        "documentation": {}
    },
    {
        "label": "group_files",
        "kind": 2,
        "importPath": "scripts.plot_active_learning",
        "description": "scripts.plot_active_learning",
        "peekOfCode": "def group_files(directory):\n    \"\"\"\n    Groups files by datasetName, LearningAlgorithm, Oracle, and foldID in a nested dictionary.\n    \"\"\"\n    files_dict = defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: defaultdict(list))))\n    # List all files in the directory\n    for filename in os.listdir(directory):\n        if filename.endswith('.csv'):\n            parsed = parse_filename(filename)\n            if parsed:",
        "detail": "scripts.plot_active_learning",
        "documentation": {}
    },
    {
        "label": "compute_spearman_correlation",
        "kind": 2,
        "importPath": "scripts.plot_active_learning",
        "description": "scripts.plot_active_learning",
        "peekOfCode": "def compute_spearman_correlation(df):\n    \"\"\"\n    Computes the Spearman correlation coefficient between rankApprox and rankOracle.\n    \"\"\"\n    correlation, _ = spearmanr(df['rankApprox'], df['rankOracle'])\n    return correlation\ndef compute_rankings(df):\n    \"\"\"\n    Computes rankings based on scoreApprox and scoreOracle.\n    Returns two Series containing the rankings.",
        "detail": "scripts.plot_active_learning",
        "documentation": {}
    },
    {
        "label": "compute_rankings",
        "kind": 2,
        "importPath": "scripts.plot_active_learning",
        "description": "scripts.plot_active_learning",
        "peekOfCode": "def compute_rankings(df):\n    \"\"\"\n    Computes rankings based on scoreApprox and scoreOracle.\n    Returns two Series containing the rankings.\n    \"\"\"\n    df = df.copy()\n    df['rankApprox'] = df['scoreApprox'].rank(ascending=False, method='first')\n    df['rankOracle'] = df['scoreOracle'].rank(ascending=False, method='first')\n    return df\ndef compute_average_precision_at_k(df, k):",
        "detail": "scripts.plot_active_learning",
        "documentation": {}
    },
    {
        "label": "compute_average_precision_at_k",
        "kind": 2,
        "importPath": "scripts.plot_active_learning",
        "description": "scripts.plot_active_learning",
        "peekOfCode": "def compute_average_precision_at_k(df, k):\n    \"\"\"\n    Computes Average Precision at top k entries.\n    \"\"\"\n    df = df.nsmallest(k, 'rankApprox')\n    relevant = df['rankOracle'] <= k\n    precision_at_k = relevant.sum() / k\n    return precision_at_k\ndef compute_regret_at_k(df, k):\n    \"\"\"",
        "detail": "scripts.plot_active_learning",
        "documentation": {}
    },
    {
        "label": "compute_regret_at_k",
        "kind": 2,
        "importPath": "scripts.plot_active_learning",
        "description": "scripts.plot_active_learning",
        "peekOfCode": "def compute_regret_at_k(df, k):\n    \"\"\"\n    Computes Regret at top k entries.\n    \"\"\"\n    top_k_approx = set(df.nsmallest(k, 'rankApprox').index)\n    top_k_oracle = set(df.nsmallest(k, 'rankOracle').index)\n    regret = len(top_k_oracle - top_k_approx) / k\n    return regret\ndef compute_unique_ratio(df, k):\n    \"\"\"",
        "detail": "scripts.plot_active_learning",
        "documentation": {}
    },
    {
        "label": "compute_unique_ratio",
        "kind": 2,
        "importPath": "scripts.plot_active_learning",
        "description": "scripts.plot_active_learning",
        "peekOfCode": "def compute_unique_ratio(df, k):\n    \"\"\"\n    Computes the percentage of unique values in df[\"scoreOracle\"].\n    \"\"\"\n    unique_values = df[\"scoreOracle\"].nunique()  \n    total_values = len(df[\"scoreOracle\"]) \n    unique_ratio = (unique_values / total_values) * 100  \n    return unique_ratio\ndef compute_kendalltau_correlation(df):\n    \"\"\"",
        "detail": "scripts.plot_active_learning",
        "documentation": {}
    },
    {
        "label": "compute_kendalltau_correlation",
        "kind": 2,
        "importPath": "scripts.plot_active_learning",
        "description": "scripts.plot_active_learning",
        "peekOfCode": "def compute_kendalltau_correlation(df):\n    \"\"\"\n    Computes the kendalltau correlation coefficient between rankApprox and rankOracle.\n    \"\"\"\n    correlation, _ = kendalltau(df['rankApprox'], df['rankOracle'])\n    return correlation\ndef process_files(grouped_files, cumulative):\n    \"\"\"\n    Processes each group of files, computes metrics, and organizes results by datasetName, then normMethod, then Oracle, then LearningAlgorithm, and foldID.\n    The metrics are stored in order based on the ascending timestamp extracted from the file names.",
        "detail": "scripts.plot_active_learning",
        "documentation": {}
    },
    {
        "label": "process_files",
        "kind": 2,
        "importPath": "scripts.plot_active_learning",
        "description": "scripts.plot_active_learning",
        "peekOfCode": "def process_files(grouped_files, cumulative):\n    \"\"\"\n    Processes each group of files, computes metrics, and organizes results by datasetName, then normMethod, then Oracle, then LearningAlgorithm, and foldID.\n    The metrics are stored in order based on the ascending timestamp extracted from the file names.\n    If cumulative is True, cumulative sums are calculated for average precision.\n    \"\"\"\n    results = defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: {'avg_precision_1': [], 'avg_precision_10': [], 'timestamps': []}))))\n    top_percentage_1 = 0.01\n    top_percentage_10 = 0.10\n    print(\"Processing files...\")",
        "detail": "scripts.plot_active_learning",
        "documentation": {}
    },
    {
        "label": "collect_all_algorithms",
        "kind": 2,
        "importPath": "scripts.plot_active_learning",
        "description": "scripts.plot_active_learning",
        "peekOfCode": "def collect_all_algorithms(results):\n    \"\"\"\n    Collects all unique algorithm names from the results.\n    \"\"\"\n    algorithms_set = set()\n    for datasetName, oracles in results.items():\n        for oracle, algorithms in oracles.items():\n                for algo in algorithms.keys():\n                    algorithms_set.add(algo)\n    return sorted(algorithms_set)",
        "detail": "scripts.plot_active_learning",
        "documentation": {}
    },
    {
        "label": "create_algorithm_color_mapping",
        "kind": 2,
        "importPath": "scripts.plot_active_learning",
        "description": "scripts.plot_active_learning",
        "peekOfCode": "def create_algorithm_color_mapping(algorithms):\n    \"\"\"\n    Creates a consistent color mapping for algorithms using a high-contrast color palette.\n    \"\"\"\n    num_algorithms = len(algorithms)\n    # Use a contrasting palette like \"Set1\" or \"bright\"\n    color_palette = sns.color_palette(\"bright\", num_algorithms)  # You can also try \"bright\" or other high-contrast palettes\n    algorithm_color_mapping = dict(zip(algorithms, color_palette))\n    return algorithm_color_mapping\nplt.rc('text', usetex=True)",
        "detail": "scripts.plot_active_learning",
        "documentation": {}
    },
    {
        "label": "save_horizontal_legend",
        "kind": 2,
        "importPath": "scripts.plot_active_learning",
        "description": "scripts.plot_active_learning",
        "peekOfCode": "def save_horizontal_legend(lines, labels, filename=\"legend.pdf\", title=\"Legend\", title_fontsize=16):\n    \"\"\"\n    Saves the legend of the provided Axes object as a horizontal PDF figure with minimal vertical space.\n    \"\"\"\n    # Replace the labels using the mapping\n    labels = [legend_label_mapping.get(label, label) for label in labels]\n    fig_legend = plt.figure(figsize=(len(labels), 0.5))  # width depends on labels, height is minimal\n    fig_legend.legend(lines, labels, loc='center', ncol=len(labels), title=title, title_fontsize=title_fontsize)\n    fig_legend.gca().set_axis_off()\n    fig_legend.savefig(filename, bbox_inches='tight')",
        "detail": "scripts.plot_active_learning",
        "documentation": {}
    },
    {
        "label": "plot_metrics",
        "kind": 2,
        "importPath": "scripts.plot_active_learning",
        "description": "scripts.plot_active_learning",
        "peekOfCode": "def plot_metrics(results, algorithm_color_mapping, cumulative):\n    \"\"\"\n    Generates and saves plots for average precision at 1% and 10% from the results data,\n    plotting metrics for each algorithm on the same graph with different colors and legend.\n    If cumulative is True, plots cumulative metrics.\n    \"\"\"\n    print(\"Plotting metrics...\")\n    for datasetName, oracles in results.items():\n        for oracle, algorithms in oracles.items():\n            print(f\"Creating plots for Dataset: {datasetName}, Oracle: {oracle}\")",
        "detail": "scripts.plot_active_learning",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "scripts.plot_active_learning",
        "description": "scripts.plot_active_learning",
        "peekOfCode": "def main():\n    parser = argparse.ArgumentParser(description='Process CSV files to compute metrics and generate plots.')\n    parser.add_argument('directory', type=str, help='Directory containing CSV files.')\n    parser.add_argument('--cumulative', action='store_true', help='Plot cumulative metrics if this flag is set.')\n    args = parser.parse_args()\n    data_dir = args.directory\n    cumulative = args.cumulative\n    if not os.path.isdir(data_dir):\n        print(f\"The directory {data_dir} does not exist or is not a directory.\")\n        sys.exit(1)",
        "detail": "scripts.plot_active_learning",
        "documentation": {}
    },
    {
        "label": "legend_label_mapping",
        "kind": 5,
        "importPath": "scripts.plot_active_learning",
        "description": "scripts.plot_active_learning",
        "peekOfCode": "legend_label_mapping = {\n    \"TopTwoRules-0.0\": r\"$\\textsc{Hss}$\",\n    \"ScoreDifference-0.0\": r\"$\\textsc{GUS-affine}$\",\n    \"BradleyTerry-0.0\": r\"$\\textsc{GUS-logistic}$\",\n    \"Thurstone-0.0\": r\"$\\textsc{GUS-cdf}$\",\n    \"ChoquetRank-0.0\": r\"$\\textsc{ChoquetRank}$\"\n}\n# Function to save the legend as a horizontal legend\ndef save_horizontal_legend(lines, labels, filename=\"legend.pdf\", title=\"Legend\", title_fontsize=16):\n    \"\"\"",
        "detail": "scripts.plot_active_learning",
        "documentation": {}
    },
    {
        "label": "parse_filename",
        "kind": 2,
        "importPath": "scripts.plot_active_normalization",
        "description": "scripts.plot_active_normalization",
        "peekOfCode": "def parse_filename(filename):\n    \"\"\"\n    Parses the filename to extract datasetName, foldID, LearningAlgorithm, normMethod, Oracle, and timestamp.\n    Expected filename format: datasetName_foldID_LearningAlgorithm-normMethod_Oracle_timestamp.csv\n    \"\"\"\n    basename = os.path.basename(filename)\n    basename = basename[:-4]  # Remove the '.csv' extension\n    parts = basename.split('_')\n    if len(parts) < 5:\n        print(f\"Filename {filename} does not match expected pattern.\")",
        "detail": "scripts.plot_active_normalization",
        "documentation": {}
    },
    {
        "label": "group_files",
        "kind": 2,
        "importPath": "scripts.plot_active_normalization",
        "description": "scripts.plot_active_normalization",
        "peekOfCode": "def group_files(directory):\n    \"\"\"\n    Groups files by datasetName, normMethod, Oracle, LearningAlgorithm, and foldID in a nested dictionary.\n    \"\"\"\n    files_dict = defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: defaultdict(list)))))\n    # List all files in the directory\n    print(\"Grouping files...\")\n    for filename in os.listdir(directory):\n        if filename.endswith('.csv'):\n            parsed = parse_filename(filename)",
        "detail": "scripts.plot_active_normalization",
        "documentation": {}
    },
    {
        "label": "compute_kendalltau_correlation",
        "kind": 2,
        "importPath": "scripts.plot_active_normalization",
        "description": "scripts.plot_active_normalization",
        "peekOfCode": "def compute_kendalltau_correlation(df):\n    \"\"\"\n    Computes the kendalltau correlation coefficient between rankApprox and rankOracle.\n    \"\"\"\n    correlation, _ = kendalltau(df['rankApprox'], df['rankOracle'])\n    return correlation\ndef compute_rankings(df):\n    \"\"\"\n    Computes rankings based on scoreApprox and scoreOracle.\n    Adds 'rankApprox' and 'rankOracle' columns to the DataFrame.",
        "detail": "scripts.plot_active_normalization",
        "documentation": {}
    },
    {
        "label": "compute_rankings",
        "kind": 2,
        "importPath": "scripts.plot_active_normalization",
        "description": "scripts.plot_active_normalization",
        "peekOfCode": "def compute_rankings(df):\n    \"\"\"\n    Computes rankings based on scoreApprox and scoreOracle.\n    Adds 'rankApprox' and 'rankOracle' columns to the DataFrame.\n    \"\"\"\n    df = df.copy()\n    df['rankApprox'] = df['scoreApprox'].rank(ascending=False, method='first')\n    df['rankOracle'] = df['scoreOracle'].rank(ascending=False, method='first')\n    return df\ndef compute_average_precision_at_k(df, k):",
        "detail": "scripts.plot_active_normalization",
        "documentation": {}
    },
    {
        "label": "compute_average_precision_at_k",
        "kind": 2,
        "importPath": "scripts.plot_active_normalization",
        "description": "scripts.plot_active_normalization",
        "peekOfCode": "def compute_average_precision_at_k(df, k):\n    \"\"\"\n    Computes Average Precision at top k entries.\n    \"\"\"\n    df_top_k = df.nsmallest(k, 'rankApprox')\n    relevant = df_top_k['rankOracle'] <= k\n    precision_at_k = relevant.sum() / k\n    return precision_at_k\ndef compute_unique_ratio(df):\n    \"\"\"",
        "detail": "scripts.plot_active_normalization",
        "documentation": {}
    },
    {
        "label": "compute_unique_ratio",
        "kind": 2,
        "importPath": "scripts.plot_active_normalization",
        "description": "scripts.plot_active_normalization",
        "peekOfCode": "def compute_unique_ratio(df):\n    \"\"\"\n    Computes the percentage of unique values in df[\"scoreOracle\"].\n    \"\"\"\n    unique_values = df[\"scoreOracle\"].nunique()\n    total_values = len(df[\"scoreOracle\"])\n    unique_ratio = (unique_values / total_values) * 100\n    return unique_ratio\ndef process_files(grouped_files):\n    \"\"\"",
        "detail": "scripts.plot_active_normalization",
        "documentation": {}
    },
    {
        "label": "process_files",
        "kind": 2,
        "importPath": "scripts.plot_active_normalization",
        "description": "scripts.plot_active_normalization",
        "peekOfCode": "def process_files(grouped_files):\n    \"\"\"\n    Processes each group of files, computes metrics, and organizes results by datasetName, then normMethod, then Oracle, then LearningAlgorithm, and foldID.\n    The metrics are stored in order based on the ascending timestamp extracted from the file names.\n    \"\"\"\n    results = defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: {'avg_precision': [], 'kendalltau': [], 'timestamps': []})))))\n    top_percentage = 0.1\n    print(\"Processing files...\")\n    for datasetName, normMethods in grouped_files.items():\n        print(f\"Dataset: {datasetName}\")",
        "detail": "scripts.plot_active_normalization",
        "documentation": {}
    },
    {
        "label": "collect_all_algorithms",
        "kind": 2,
        "importPath": "scripts.plot_active_normalization",
        "description": "scripts.plot_active_normalization",
        "peekOfCode": "def collect_all_algorithms(results):\n    \"\"\"\n    Collects all unique algorithm names from the results.\n    \"\"\"\n    algorithms_set = set()\n    for datasetName, normMethods in results.items():\n        for normMethod, oracles in normMethods.items():\n            for oracle, algorithms in oracles.items():\n                for algo in algorithms.keys():\n                    algorithms_set.add(algo)",
        "detail": "scripts.plot_active_normalization",
        "documentation": {}
    },
    {
        "label": "create_algorithm_color_mapping",
        "kind": 2,
        "importPath": "scripts.plot_active_normalization",
        "description": "scripts.plot_active_normalization",
        "peekOfCode": "def create_algorithm_color_mapping(algorithms):\n    \"\"\"\n    Creates a consistent color mapping for algorithms.\n    \"\"\"\n    num_algorithms = len(algorithms)\n    color_palette = sns.color_palette(\"deep\", num_algorithms)\n    algorithm_color_mapping = dict(zip(algorithms, color_palette))\n    return algorithm_color_mapping\ndef plot_metrics(results, algorithm_color_mapping):\n    \"\"\"",
        "detail": "scripts.plot_active_normalization",
        "documentation": {}
    },
    {
        "label": "plot_metrics",
        "kind": 2,
        "importPath": "scripts.plot_active_normalization",
        "description": "scripts.plot_active_normalization",
        "peekOfCode": "def plot_metrics(results, algorithm_color_mapping):\n    \"\"\"\n    Generates and saves plots for average precision and kendalltau correlation from the results data,\n    plotting metrics for each algorithm on the same graph with different colors and legend.\n    Parameters:\n        results (dict): The dictionary containing the nested results from process_files.\n        algorithm_color_mapping (dict): A dictionary mapping algorithms to specific colors.\n    \"\"\"\n    print(\"Plotting metrics...\")\n    for datasetName, normMethods in results.items():",
        "detail": "scripts.plot_active_normalization",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "scripts.plot_active_normalization",
        "description": "scripts.plot_active_normalization",
        "peekOfCode": "def main():\n    parser = argparse.ArgumentParser(description='Process CSV files to compute metrics and generate plots.')\n    parser.add_argument('directory', type=str, help='Directory containing CSV files.')\n    args = parser.parse_args()\n    data_dir = args.directory\n    if not os.path.isdir(data_dir):\n        print(f\"The directory {data_dir} does not exist or is not a directory.\")\n        sys.exit(1)\n    grouped_files = group_files(data_dir)\n    if not grouped_files:",
        "detail": "scripts.plot_active_normalization",
        "documentation": {}
    },
    {
        "label": "group_files_in_nested_dict",
        "kind": 2,
        "importPath": "scripts.plot_sampling",
        "description": "scripts.plot_sampling",
        "peekOfCode": "def group_files_in_nested_dict(base_folder):\n    grouped_files = defaultdict(lambda: defaultdict(lambda: defaultdict(list)))\n    for folder in os.listdir(base_folder):\n        folder_path = os.path.join(base_folder, folder)\n        if os.path.isdir(folder_path):\n            for filename in os.listdir(folder_path):\n                if filename.endswith(\".csv\"):\n                    filename_without_extension = filename[:-4]\n                    parts = filename_without_extension.split('_')\n                    timestamp = parts[-1]",
        "detail": "scripts.plot_sampling",
        "documentation": {}
    },
    {
        "label": "compute_mean_variance_for_best_scores",
        "kind": 2,
        "importPath": "scripts.plot_sampling",
        "description": "scripts.plot_sampling",
        "peekOfCode": "def compute_mean_variance_for_best_scores(grouped_files):\n    results = defaultdict(lambda: defaultdict(lambda: {}))\n    for datasetName, outRankingDict in grouped_files.items():\n        for outRankingCertainty, numIterationDict in outRankingDict.items():\n            for numIterations, files in numIterationDict.items():\n                best_scores = []\n                for file in files:\n                    try:\n                        df = pd.read_csv(file)\n                        if df.empty:",
        "detail": "scripts.plot_sampling",
        "documentation": {}
    },
    {
        "label": "save_horizontal_legend",
        "kind": 2,
        "importPath": "scripts.plot_sampling",
        "description": "scripts.plot_sampling",
        "peekOfCode": "def save_horizontal_legend(ax, filename=\"legend.pdf\", title=\"Legend\", title_fontsize=16):\n    \"\"\"\n    Saves the legend of the provided Axes object as a horizontal PDF figure with minimal vertical space.\n    \"\"\"\n    lines, labels = ax.get_legend_handles_labels()\n    # Replace the labels using the mapping\n    labels = [legend_label_mapping.get(label, label) for label in labels]\n    fig_legend = plt.figure(figsize=(len(labels), 0.5))  # width depends on labels, height is minimal\n    fig_legend.legend(lines, labels, loc='center', ncol=len(labels), title=title, title_fontsize=title_fontsize)\n    fig_legend.gca().set_axis_off()",
        "detail": "scripts.plot_sampling",
        "documentation": {}
    },
    {
        "label": "plot_results_and_save",
        "kind": 2,
        "importPath": "scripts.plot_sampling",
        "description": "scripts.plot_sampling",
        "peekOfCode": "def plot_results_and_save(mean_variance_results, grouped_files, output_folder):\n    for datasetName, outRankingDict in mean_variance_results.items():\n        fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20, 6))  # Create three subplots side by side\n        # First plot (ax1): Maximum score vs iterations with error bars\n        for outRankingCertainty, numIterationDict in outRankingDict.items():\n            num_iterations = []\n            means = []\n            errors = []\n            for numIterations, stats in sorted(numIterationDict.items(), key=lambda x: int(x[0])):\n                num_iterations.append(int(numIterations))",
        "detail": "scripts.plot_sampling",
        "documentation": {}
    },
    {
        "label": "legend_label_mapping",
        "kind": 5,
        "importPath": "scripts.plot_sampling",
        "description": "scripts.plot_sampling",
        "peekOfCode": "legend_label_mapping = {\n    \"ScoreDifferenceOutRanking\": r\"$\\textsc{SiMAS-affine}$\",\n    \"ThurstoneOutRanking\": r\"$\\textsc{Simas-cdf}$\",\n    \"BradleyTerryOutRanking\": r\"$\\textsc{Simas-logistic}$\",\n    \"UnrestrictedSampling\": r\"$\\textsc{UnrestrictedSampling}$\"\n}\n# Updated save_horizontal_legend function with LaTeX support\ndef save_horizontal_legend(ax, filename=\"legend.pdf\", title=\"Legend\", title_fontsize=16):\n    \"\"\"\n    Saves the legend of the provided Axes object as a horizontal PDF figure with minimal vertical space.",
        "detail": "scripts.plot_sampling",
        "documentation": {}
    },
    {
        "label": "base_folder",
        "kind": 5,
        "importPath": "scripts.plot_sampling",
        "description": "scripts.plot_sampling",
        "peekOfCode": "base_folder = \"results/sampling_experiment/samples/ratio_1to1/\"\n# Group files\nnested_grouped_files = group_files_in_nested_dict(base_folder)\n# Compute mean and variance for each numIterations group\nmean_variance_results = compute_mean_variance_for_best_scores(nested_grouped_files)\n# Define the output folder where the plots will be saved\noutput_folder = \"results/sampling_experiment/output_plots/ratio_1to1\"\nos.makedirs(output_folder, exist_ok=True)\n# Call the updated plot function to generate and save the side-by-side plots\nplot_results_and_save(mean_variance_results, nested_grouped_files, output_folder)",
        "detail": "scripts.plot_sampling",
        "documentation": {}
    },
    {
        "label": "nested_grouped_files",
        "kind": 5,
        "importPath": "scripts.plot_sampling",
        "description": "scripts.plot_sampling",
        "peekOfCode": "nested_grouped_files = group_files_in_nested_dict(base_folder)\n# Compute mean and variance for each numIterations group\nmean_variance_results = compute_mean_variance_for_best_scores(nested_grouped_files)\n# Define the output folder where the plots will be saved\noutput_folder = \"results/sampling_experiment/output_plots/ratio_1to1\"\nos.makedirs(output_folder, exist_ok=True)\n# Call the updated plot function to generate and save the side-by-side plots\nplot_results_and_save(mean_variance_results, nested_grouped_files, output_folder)",
        "detail": "scripts.plot_sampling",
        "documentation": {}
    },
    {
        "label": "mean_variance_results",
        "kind": 5,
        "importPath": "scripts.plot_sampling",
        "description": "scripts.plot_sampling",
        "peekOfCode": "mean_variance_results = compute_mean_variance_for_best_scores(nested_grouped_files)\n# Define the output folder where the plots will be saved\noutput_folder = \"results/sampling_experiment/output_plots/ratio_1to1\"\nos.makedirs(output_folder, exist_ok=True)\n# Call the updated plot function to generate and save the side-by-side plots\nplot_results_and_save(mean_variance_results, nested_grouped_files, output_folder)",
        "detail": "scripts.plot_sampling",
        "documentation": {}
    },
    {
        "label": "output_folder",
        "kind": 5,
        "importPath": "scripts.plot_sampling",
        "description": "scripts.plot_sampling",
        "peekOfCode": "output_folder = \"results/sampling_experiment/output_plots/ratio_1to1\"\nos.makedirs(output_folder, exist_ok=True)\n# Call the updated plot function to generate and save the side-by-side plots\nplot_results_and_save(mean_variance_results, nested_grouped_files, output_folder)",
        "detail": "scripts.plot_sampling",
        "documentation": {}
    },
    {
        "label": "read_rules",
        "kind": 2,
        "importPath": "scripts.plot_search_space",
        "description": "scripts.plot_search_space",
        "peekOfCode": "def read_rules(file_path):\n    rules = []\n    scores = {}\n    raw_scores = []\n    with open(file_path, newline='') as csvfile:\n        reader = csv.DictReader(csvfile)\n        for row in reader:\n            antecedent = tuple(sorted(map(int, row['antecedent'].strip('{}').split(';'))))\n            consequent = tuple(sorted(map(int, row['consequent'].strip('{}').split(';'))))\n            rule = (antecedent, consequent)",
        "detail": "scripts.plot_search_space",
        "documentation": {}
    },
    {
        "label": "out_ranking_certainty",
        "kind": 2,
        "importPath": "scripts.plot_search_space",
        "description": "scripts.plot_search_space",
        "peekOfCode": "def out_ranking_certainty(scoreA, scoreB):\n    csi = 10\n    # return (scoreA - scoreB) / 2 + 0.5\n    return np.exp(csi*scoreA) / (np.exp(csi*scoreA) + np.exp(csi*scoreB)) \ndef create_graph(rules, scores):\n    G = nx.Graph()\n    for rule in rules:\n        G.add_node(rule, score=scores[rule], size=300)  # Fixed size for all nodes\n    for i, rule1 in enumerate(rules):\n        neighbors = []",
        "detail": "scripts.plot_search_space",
        "documentation": {}
    },
    {
        "label": "create_graph",
        "kind": 2,
        "importPath": "scripts.plot_search_space",
        "description": "scripts.plot_search_space",
        "peekOfCode": "def create_graph(rules, scores):\n    G = nx.Graph()\n    for rule in rules:\n        G.add_node(rule, score=scores[rule], size=300)  # Fixed size for all nodes\n    for i, rule1 in enumerate(rules):\n        neighbors = []\n        for j, rule2 in enumerate(rules):\n            if i != j and are_neighbors(rule1, rule2):\n                neighbors.append(rule2)\n        for rule2 in neighbors:",
        "detail": "scripts.plot_search_space",
        "documentation": {}
    },
    {
        "label": "compute_statistics",
        "kind": 2,
        "importPath": "scripts.plot_search_space",
        "description": "scripts.plot_search_space",
        "peekOfCode": "def compute_statistics(G):\n    num_nodes = G.number_of_nodes()\n    num_edges = G.number_of_edges()\n    if num_nodes > 1:\n        connectivity_ratio = num_edges / (num_nodes * (num_nodes - 1) / 2)\n    else:\n        connectivity_ratio = 0\n    # Retrieve edge weights from the graph\n    edge_weights = [G[u][v]['weight'] for u, v in G.edges()]\n    avg_edge_weight = np.mean(edge_weights) if edge_weights else 0",
        "detail": "scripts.plot_search_space",
        "documentation": {}
    },
    {
        "label": "are_neighbors",
        "kind": 2,
        "importPath": "scripts.plot_search_space",
        "description": "scripts.plot_search_space",
        "peekOfCode": "def are_neighbors(rule1, rule2):\n    antecedent1, _ = rule1\n    antecedent2, _ = rule2\n    return len(set(antecedent1).symmetric_difference(set(antecedent2))) == 1\ndef plot_graph(G):\n    pos = nx.spring_layout(G)\n    # Prepare edge colors with varying alpha values based on edge weights\n    edge_weights = nx.get_edge_attributes(G, 'weight')\n    min_weight, max_weight = min(edge_weights.values()), max(edge_weights.values())\n    edge_alphas = [(edge_weights[(u, v)] - min_weight) / (max_weight - min_weight) for u, v in G.edges()]  # Normalize weights for alpha",
        "detail": "scripts.plot_search_space",
        "documentation": {}
    },
    {
        "label": "plot_graph",
        "kind": 2,
        "importPath": "scripts.plot_search_space",
        "description": "scripts.plot_search_space",
        "peekOfCode": "def plot_graph(G):\n    pos = nx.spring_layout(G)\n    # Prepare edge colors with varying alpha values based on edge weights\n    edge_weights = nx.get_edge_attributes(G, 'weight')\n    min_weight, max_weight = min(edge_weights.values()), max(edge_weights.values())\n    edge_alphas = [(edge_weights[(u, v)] - min_weight) / (max_weight - min_weight) for u, v in G.edges()]  # Normalize weights for alpha\n    node_scores = nx.get_node_attributes(G, 'score')\n    min_score, max_score = min(node_scores.values()), max(node_scores.values())\n    node_colors = [plt.cm.RdBu((score - min_score) / (max_score - min_score)) for score in node_scores.values()]  # Color nodes from blue to red based on scores\n    # Draw nodes",
        "detail": "scripts.plot_search_space",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "scripts.plot_search_space",
        "description": "scripts.plot_search_space",
        "peekOfCode": "def main():\n    if len(sys.argv) != 2:\n        print(\"Usage: python script.py <path_to_csv_file>\")\n        sys.exit(1)\n    file_path = sys.argv[1]\n    rules, scores = read_rules(file_path)\n    G = create_graph(rules, scores)\n    compute_statistics(G)\n    plot_graph(G)\nif __name__ == \"__main__\":",
        "detail": "scripts.plot_search_space",
        "documentation": {}
    },
    {
        "label": "parse_arguments",
        "kind": 2,
        "importPath": "scripts.prepare_data",
        "description": "scripts.prepare_data",
        "peekOfCode": "def parse_arguments():\n    parser = argparse.ArgumentParser(description='Process datasets for cross-validation and rule mining.')\n    parser.add_argument('data_folder_path', type=str, help='Path to the data folder containing .dat files')\n    parser.add_argument('num_folds', type=int, help='Number of folds for cross-validation')\n    parser.add_argument('output_folder_path', type=str, help='Output path for processed datasets and rules')\n    return parser.parse_args()\ndef read_dat_file(file_path):\n    try:\n        data = pd.read_csv(file_path, header=None)\n        return data",
        "detail": "scripts.prepare_data",
        "documentation": {}
    },
    {
        "label": "read_dat_file",
        "kind": 2,
        "importPath": "scripts.prepare_data",
        "description": "scripts.prepare_data",
        "peekOfCode": "def read_dat_file(file_path):\n    try:\n        data = pd.read_csv(file_path, header=None)\n        return data\n    except Exception as e:\n        print(f\"Error reading {file_path}: {e}\")\n        return None\ndef process_dataset(file_path, args):\n    dataset_name = os.path.splitext(os.path.basename(file_path))[0]\n    data = read_dat_file(file_path)",
        "detail": "scripts.prepare_data",
        "documentation": {}
    },
    {
        "label": "process_dataset",
        "kind": 2,
        "importPath": "scripts.prepare_data",
        "description": "scripts.prepare_data",
        "peekOfCode": "def process_dataset(file_path, args):\n    dataset_name = os.path.splitext(os.path.basename(file_path))[0]\n    data = read_dat_file(file_path)\n    if data is None:\n        return\n    # Prepare output directories\n    dataset_output_path = os.path.join(args.output_folder_path, dataset_name)\n    # Create train and test subdirectories if they do not exist\n    train_output_path = os.path.join(dataset_output_path, 'train')\n    test_output_path = os.path.join(dataset_output_path, 'test')",
        "detail": "scripts.prepare_data",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "scripts.prepare_data",
        "description": "scripts.prepare_data",
        "peekOfCode": "def main():\n    args = parse_arguments()\n    # Process each .dat file in the data folder\n    for file_name in os.listdir(args.data_folder_path):\n        if file_name.endswith('.dat'):\n            file_path = os.path.join(args.data_folder_path, file_name)\n            process_dataset(file_path, args)\nif __name__ == '__main__':\n    main()",
        "detail": "scripts.prepare_data",
        "documentation": {}
    }
]